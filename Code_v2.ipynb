{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "bert-babble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivDbj1g_oZXq",
        "colab_type": "code",
        "outputId": "66288935-9b04-43be-c113-28832a961d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MH0KOaXZoV0Y",
        "colab_type": "code",
        "outputId": "4763ce24-595e-4118-97b3-c8f196bcf91c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ggyJPg-3oV0g",
        "colab_type": "code",
        "outputId": "ddb8b644-e1a7-4c21-e9e2-8d630f3f6108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda(0)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:14<00:00, 27258183.66B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 922446.29B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txbo237GoV0l",
        "colab_type": "text"
      },
      "source": [
        "# Generations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md37PpWpoV0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "\n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None: \n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(\n",
        "            dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24bJauOQfRNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_tknized(w):\n",
        "    random.shuffle(w)\n",
        "    i = 0\n",
        "    while(i < len(w)):\n",
        "        try:\n",
        "            wi = tokenizer.convert_tokens_to_ids([w[i]])[0]\n",
        "            print(tokenizer.convert_ids_to_tokens([wi]))\n",
        "            return wi\n",
        "            break\n",
        "        except:\n",
        "            i += 1\n",
        "\n",
        "\n",
        "def add_context(batch):\n",
        "    w = list(set(sum([ss.lemma_names() for ss in wordnet.synsets(WORD)], [])))\n",
        "    print(w)\n",
        "    for b in batch:\n",
        "        ixs = random.sample(range(1, max_len+1), 1)\n",
        "        for i in ixs:\n",
        "            b[i] = get_word_tknized(w)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLT6BNN9oV0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation modes as functions\n",
        "def get_init_text(seed_text, max_len, batch_size=1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    # if rand_init:\n",
        "    #    for ii in range(max_len):\n",
        "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
        "    batch = tokenize_batch(batch)\n",
        "    batch = add_context(batch)\n",
        "    print(batch)\n",
        "    return batch\n",
        "\n",
        "\n",
        "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200, cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "\n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "\n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk,\n",
        "                             temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "\n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + \\\n",
        "                ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "\n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25,\n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
        "                                               temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
        "                                               cuda=cuda, verbose=False)\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\n",
        "                f\"Finished batch {(batch_n + 1)} in {(time.time() - start_time)}s\")\n",
        "            start_time = time.time()\n",
        "\n",
        "        sentences += batch\n",
        "    return sentences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsV2vg4IoV0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "    with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_jlQRwVoV0z",
        "colab_type": "code",
        "outputId": "65a77dc9-3224-4897-b339-a824a2a27beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "    n_samples = 10\n",
        "    batch_size = 5\n",
        "    max_len = 20\n",
        "    top_k = 100\n",
        "    temperature = 0.7\n",
        "\n",
        "    burnin = 250\n",
        "    sample = True\n",
        "    max_iter = 499\n",
        "    WORD = 'Sad'\n",
        "\n",
        "    \n",
        "    # Choose the prefix context\n",
        "    # seed_text = f\"[CLS] {w[1]}\".split()\n",
        "    seed_text = \"[CLS]\".split()\n",
        "\n",
        "    for temp in [1.0]:\n",
        "        bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                              sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                              cuda=True)\n",
        "        out_file = \"./Test%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (\n",
        "            model_version, max_len, burnin, top_k, temp)\n",
        "        write_sents(out_file, bert_sents, should_detokenize=True)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sad', 'sorry', 'lamentable', 'pitiful', 'deplorable', 'distressing']\n",
            "['sad']\n",
            "['sorry']\n",
            "['sorry']\n",
            "['sad']\n",
            "['sad']\n",
            "[[101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 6517, 103, 103, 103, 103, 103, 103, 103, 102], [101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 3374, 103, 103, 103, 103, 102], [101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 3374, 103, 103, 103, 103, 103, 103, 103, 103, 102], [101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 6517, 103, 102], [101, 103, 103, 103, 103, 103, 6517, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 102]]\n",
            "Finished batch 1 in 13.13511347770691s\n",
            "['sad', 'sorry', 'lamentable', 'pitiful', 'deplorable', 'distressing']\n",
            "['sad']\n",
            "['sad']\n",
            "['sorry']\n",
            "['sad']\n",
            "['sad']\n",
            "[[101, 103, 103, 103, 103, 6517, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 102], [101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 6517, 103, 103, 103, 103, 103, 103, 103, 102], [101, 103, 103, 103, 103, 103, 103, 103, 103, 3374, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 102], [101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 6517, 103, 103, 103, 103, 103, 103, 103, 103, 103, 102], [101, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 6517, 103, 103, 103, 103, 103, 102]]\n",
            "Finished batch 2 in 13.060770750045776s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r61BqMqoV03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "cc5ef695-e423-48e4-ac13-b01924317780"
      },
      "source": [
        "in_file = \"Testgenerations-len20-burnin200-temp0.700.txt\"\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-aa6bdc0141fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0min_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Testgenerations-len20-burnin200-temp0.700.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbert_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-98bcd52f5e09>\u001b[0m in \u001b[0;36mread_sents\u001b[0;34m(in_file, should_detokenize)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Testgenerations-len20-burnin200-temp0.700.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oPvKcf98oV06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(50):\n",
        "    printer(sents[i], should_detokenize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHu1M3d6oV09",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LRh52kWoV0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate import bleu_score as bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOfQD4ZzoV1C",
        "colab_type": "text"
      },
      "source": [
        "## Quality Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u4mbW4RoV1D",
        "colab_type": "text"
      },
      "source": [
        "How similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., (2017) and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKKx5c4LoV1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):        \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4j0my3oV1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki103_file = 'data/wiki103.5k.txt'\n",
        "tbc_file = 'data/tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)\n",
        "#sents = [detokenize(sent) for sent in sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFZx68naoV1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data)))\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, wiki_data)))\n",
        "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yByxPJbOoV1O",
        "colab_type": "text"
      },
      "source": [
        "## Comparing to existing models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpRhDe4LoV1P",
        "colab_type": "text"
      },
      "source": [
        "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See [this repo](https://github.com/huggingface/pytorch-openai-transformer-lm) by Thomas Wolf at Huggingface for instructions for setting up the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HiTIJIoV1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(1, os.path.join(\".\", \"pytorch-openai-transformer-lm\"))\n",
        "\n",
        "from model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
        "from text_utils import TextEncoder\n",
        "\n",
        "def load_openai_gpt(n_special=1, n_ctx=512):\n",
        "    text_encoder = TextEncoder(\"pytorch-openai-transformer-lm/model/encoder_bpe_40000.json\", \n",
        "                               \"pytorch-openai-transformer-lm/model/vocab_40000.bpe\")\n",
        "    encoder = text_encoder.encoder\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    vocab = n_vocab + n_special + n_ctx\n",
        "\n",
        "    args = DEFAULT_CONFIG\n",
        "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
        "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
        "                                 path=\"pytorch-openai-transformer-lm/model/\",\n",
        "                                 path_names=\"pytorch-openai-transformer-lm/\")\n",
        "    #lm_model.to(device)\n",
        "    lm_model.return_probs = False\n",
        "    lm_model.eval()\n",
        "    return lm_model, text_encoder\n",
        "\n",
        "def make_batch(X, n_vocab, n_special, batch_size):\n",
        "    X = np.array(X)\n",
        "    assert X.ndim in [1, 2]\n",
        "    if X.ndim == 1:\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
        "    pos_enc = np.tile(pos_enc, (batch_size, pos_enc.shape[-1])) #np.expand_dims(pos_enc, axis=0)\n",
        "    batch = np.stack([X, pos_enc], axis=-1)\n",
        "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
        "    return batch\n",
        "\n",
        "def append_batch(X, next_idx):\n",
        "    next_pos = X[:, -1:, 1] + 1\n",
        "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
        "    return torch.cat((X, next_x), 1)\n",
        "\n",
        "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
        "                             topk=100, sample=True, n_special=0):\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
        "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
        "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
        "    sents = [[] for _ in range(batch_size)]\n",
        "    if seed_text:\n",
        "        seed_ids = text_encoder.encode([seed_text,])\n",
        "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
        "        sents = [[seed_text] for _ in range(batch_size)]\n",
        "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    for step_n in range(gen_len):\n",
        "        out = model(XMB) + model.pos_emb_mask\n",
        "        next_idxs = generate_step(out, gen_idx=step_n, top_k=topk, sample=sample, return_list=False)\n",
        "        idxs = next_idxs.tolist()\n",
        "        for i in range(batch_size):\n",
        "            next_token = idxs[i]\n",
        "            if next_token == n_vocab:\n",
        "                next_token = \"<EOS>\"\n",
        "            else:\n",
        "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
        "            sents[i].append(next_token)\n",
        "        XMB = append_batch(XMB, next_idxs.unsqueeze(-1))\n",
        "        \n",
        "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
        "\n",
        "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
        "                    batch_size=10, gen_len=20, \n",
        "                    topk=100, temperature=temperature, sample=sample,\n",
        "                    n_special=0, print_every=1):\n",
        "    sents = []\n",
        "    start_time = time.time()\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    for batch_n in range(n_batches):\n",
        "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
        "                                                batch_size=batch_size, gen_len=gen_len, \n",
        "                                                topk=topk, sample=sample,\n",
        "                                                n_special=n_special)\n",
        "        sents += batch_sents\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "trKv9zwOoV1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZYjukfV1oV1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"\", \n",
        "                               n_samples=n_samples, batch_size=batch_size, gen_len=max_len,\n",
        "                               topk=top_k, temperature=temperature, sample=sample,\n",
        "                               n_special=1, print_every=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZENMeGyoV1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "printer(openai_sents[9], should_detokenize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Mwi5TxyeoV1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"GPT-TBC BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data)))\n",
        "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, wiki_data)))\n",
        "print(\"GPT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxSv4_bwoV1e",
        "colab_type": "text"
      },
      "source": [
        "## Diversity Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJx7tBxsoV1f",
        "colab_type": "text"
      },
      "source": [
        "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyBbQoU7oV1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def self_bleu(sents):\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
        "\n",
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {}\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "\n",
        "def ref_unique_ngrams(preds, refs, max_n=4):\n",
        "    # get # of *distinct* pred ngrams that don't appear in ref\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "        pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "    return pct_unique\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "    return pct_unique"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PRdluBPHoV1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BERT self-BLEU: %.2f\" % (100 * self_bleu(bert_sents)))\n",
        "print(\"OpenAI self-BLEU: %.2f\" % (100 * self_bleu(openai_sents)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVqDJobcoV1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSCIEijboV1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pct_uniques = ref_unique_ngrams(openai_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = ref_unique_ngrams(openai_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = self_unique_ngrams(openai_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}