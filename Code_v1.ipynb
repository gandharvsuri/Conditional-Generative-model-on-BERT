{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "bert-babble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivDbj1g_oZXq",
        "colab_type": "code",
        "outputId": "0938d2c6-8322-4c2d-9119-c1c060f1ef3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 25.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MH0KOaXZoV0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3261398c-e1cc-43d1-d0c2-92b81416c91f"
      },
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ggyJPg-3oV0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5246c6ff-f628-4046-fd1c-7a99d68d5018"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda(0)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:10<00:00, 39484342.06B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 1209209.94B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txbo237GoV0l",
        "colab_type": "text"
      },
      "source": [
        "# Generations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md37PpWpoV0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24bJauOQfRNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_tknized(w):\n",
        "    random.shuffle(w)\n",
        "    i = 0\n",
        "    while(i < len(w)):\n",
        "        try:\n",
        "            wi = tokenizer.convert_tokens_to_ids([w[i]])[0]\n",
        "            print(tokenizer.convert_ids_to_tokens([wi]))\n",
        "            return wi\n",
        "            break\n",
        "        except:\n",
        "            i += 1\n",
        "\n",
        "\n",
        "def add_context(batch):\n",
        "    w = list(set(sum([ss.lemma_names() for ss in wordnet.synsets(WORD)], [])))\n",
        "    print(w)\n",
        "    for b in batch:\n",
        "        ixs = random.sample(range(1, max_len+1), 1)\n",
        "        for i in ixs:\n",
        "            b[i] = get_word_tknized(w)\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLT6BNN9oV0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    batch = add_context(batch)\n",
        "    #if rand_init:\n",
        "    #    for ii in range(max_len):\n",
        "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
        "    \n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
        "                        cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for all positions at a time step \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        for kk in range(max_len):\n",
        "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
        "            for jj in range(batch_size):\n",
        "                batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii, print_every) == 0:\n",
        "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
        "    \n",
        "    return untokenize_batch(batch)\n",
        "            \n",
        "def sequential_generation(seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
        "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
        "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    batch = batch.cuda() if cuda else batch\n",
        "    \n",
        "    for ii in range(max_len):\n",
        "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+ii] = idxs[jj]\n",
        "        \n",
        "        return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
        "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                               cuda=cuda, verbose=False)\n",
        "        \n",
        "        #batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\n",
        "        #batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\n",
        "        \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsV2vg4IoV0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "    def write_sents(out_file, sents, should_detokenize=False):\n",
        "      with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_jlQRwVoV0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "b892f145-087a-4cc6-e814-76172a62805c"
      },
      "source": [
        "n_samples = 10\n",
        "batch_size = 1\n",
        "max_len = 20 \n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "WORD = \"sad\"\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,sample=sample, top_k=top_k, \n",
        "                          temperature=temp, burnin=burnin, max_iter=max_iter,cuda=True)\n",
        "    out_file = \"./data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lamentable', 'distressing', 'deplorable', 'sorry', 'sad', 'pitiful']\n",
            "['sorry']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-146b1dd72ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,sample=sample, top_k=top_k, \n\u001b[0;32m---> 18\u001b[0;31m                           temperature=temp, burnin=burnin, max_iter=max_iter,cuda=True)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mwrite_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-cfe1ddb6d819>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(n_samples, seed_text, batch_size, max_len, sample, top_k, temperature, burnin, max_iter, cuda, print_every)\u001b[0m\n\u001b[1;32m     87\u001b[0m         batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n\u001b[1;32m     88\u001b[0m                                                \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                                cuda=cuda, verbose=False)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m#batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-cfe1ddb6d819>\u001b[0m in \u001b[0;36mparallel_sequential_generation\u001b[0;34m(seed_text, max_len, top_k, temperature, max_iter, burnin, cuda, print_every, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0mseed_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_init_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-cfe1ddb6d819>\u001b[0m in \u001b[0;36mget_init_text\u001b[0;34m(seed_text, max_len, batch_size, rand_init)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
            "\u001b[0;32m<ipython-input-3-beace4564740>\u001b[0m in \u001b[0;36mtokenize_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muntokenize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-beace4564740>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muntokenize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/tokenization.py\u001b[0m in \u001b[0;36mconvert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             logger.warning(\n",
            "\u001b[0;31mKeyError\u001b[0m: 3374"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r61BqMqoV03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "in_file = \"data/generations-len20-burnin200-temp0.700.txt\"\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oPvKcf98oV06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(50):\n",
        "    printer(sents[i], should_detokenize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHu1M3d6oV09",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LRh52kWoV0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate import bleu_score as bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOfQD4ZzoV1C",
        "colab_type": "text"
      },
      "source": [
        "## Quality Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u4mbW4RoV1D",
        "colab_type": "text"
      },
      "source": [
        "How similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., (2017) and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKKx5c4LoV1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):        \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG4j0my3oV1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wiki103_file = 'data/wiki103.5k.txt'\n",
        "tbc_file = 'data/tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)\n",
        "#sents = [detokenize(sent) for sent in sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFZx68naoV1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data)))\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, wiki_data)))\n",
        "print(\"BERT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(bert_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yByxPJbOoV1O",
        "colab_type": "text"
      },
      "source": [
        "## Comparing to existing models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpRhDe4LoV1P",
        "colab_type": "text"
      },
      "source": [
        "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See [this repo](https://github.com/huggingface/pytorch-openai-transformer-lm) by Thomas Wolf at Huggingface for instructions for setting up the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HiTIJIoV1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(1, os.path.join(\".\", \"pytorch-openai-transformer-lm\"))\n",
        "\n",
        "from model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
        "from text_utils import TextEncoder\n",
        "\n",
        "def load_openai_gpt(n_special=1, n_ctx=512):\n",
        "    text_encoder = TextEncoder(\"pytorch-openai-transformer-lm/model/encoder_bpe_40000.json\", \n",
        "                               \"pytorch-openai-transformer-lm/model/vocab_40000.bpe\")\n",
        "    encoder = text_encoder.encoder\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    vocab = n_vocab + n_special + n_ctx\n",
        "\n",
        "    args = DEFAULT_CONFIG\n",
        "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
        "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
        "                                 path=\"pytorch-openai-transformer-lm/model/\",\n",
        "                                 path_names=\"pytorch-openai-transformer-lm/\")\n",
        "    #lm_model.to(device)\n",
        "    lm_model.return_probs = False\n",
        "    lm_model.eval()\n",
        "    return lm_model, text_encoder\n",
        "\n",
        "def make_batch(X, n_vocab, n_special, batch_size):\n",
        "    X = np.array(X)\n",
        "    assert X.ndim in [1, 2]\n",
        "    if X.ndim == 1:\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
        "    pos_enc = np.tile(pos_enc, (batch_size, pos_enc.shape[-1])) #np.expand_dims(pos_enc, axis=0)\n",
        "    batch = np.stack([X, pos_enc], axis=-1)\n",
        "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
        "    return batch\n",
        "\n",
        "def append_batch(X, next_idx):\n",
        "    next_pos = X[:, -1:, 1] + 1\n",
        "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
        "    return torch.cat((X, next_x), 1)\n",
        "\n",
        "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
        "                             topk=100, sample=True, n_special=0):\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
        "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
        "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
        "    sents = [[] for _ in range(batch_size)]\n",
        "    if seed_text:\n",
        "        seed_ids = text_encoder.encode([seed_text,])\n",
        "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
        "        sents = [[seed_text] for _ in range(batch_size)]\n",
        "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    for step_n in range(gen_len):\n",
        "        out = model(XMB) + model.pos_emb_mask\n",
        "        next_idxs = generate_step(out, gen_idx=step_n, top_k=topk, sample=sample, return_list=False)\n",
        "        idxs = next_idxs.tolist()\n",
        "        for i in range(batch_size):\n",
        "            next_token = idxs[i]\n",
        "            if next_token == n_vocab:\n",
        "                next_token = \"<EOS>\"\n",
        "            else:\n",
        "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
        "            sents[i].append(next_token)\n",
        "        XMB = append_batch(XMB, next_idxs.unsqueeze(-1))\n",
        "        \n",
        "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
        "\n",
        "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
        "                    batch_size=10, gen_len=20, \n",
        "                    topk=100, temperature=temperature, sample=sample,\n",
        "                    n_special=0, print_every=1):\n",
        "    sents = []\n",
        "    start_time = time.time()\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    for batch_n in range(n_batches):\n",
        "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
        "                                                batch_size=batch_size, gen_len=gen_len, \n",
        "                                                topk=topk, sample=sample,\n",
        "                                                n_special=n_special)\n",
        "        sents += batch_sents\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "    return sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "trKv9zwOoV1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZYjukfV1oV1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"\", \n",
        "                               n_samples=n_samples, batch_size=batch_size, gen_len=max_len,\n",
        "                               topk=top_k, temperature=temperature, sample=sample,\n",
        "                               n_special=1, print_every=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZENMeGyoV1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "printer(openai_sents[9], should_detokenize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Mwi5TxyeoV1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"GPT-TBC BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data)))\n",
        "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, wiki_data)))\n",
        "print(\"GPT-{TBC + Wiki103} BLEU: %.2f\" % (100 * corpus_bleu(openai_sents, tbc_data[:2500] + wiki_data[:2500])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxSv4_bwoV1e",
        "colab_type": "text"
      },
      "source": [
        "## Diversity Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJx7tBxsoV1f",
        "colab_type": "text"
      },
      "source": [
        "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyBbQoU7oV1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def self_bleu(sents):\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
        "\n",
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {}\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "\n",
        "def ref_unique_ngrams(preds, refs, max_n=4):\n",
        "    # get # of *distinct* pred ngrams that don't appear in ref\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "        pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "    return pct_unique\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "    return pct_unique"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PRdluBPHoV1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"BERT self-BLEU: %.2f\" % (100 * self_bleu(bert_sents)))\n",
        "print(\"OpenAI self-BLEU: %.2f\" % (100 * self_bleu(openai_sents)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVqDJobcoV1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSCIEijboV1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pct_uniques = ref_unique_ngrams(openai_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = ref_unique_ngrams(openai_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "pct_uniques = self_unique_ngrams(openai_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}